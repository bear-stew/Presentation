# Presentation


 | Paper | Conference | Year | Task | Presenter | Presentation | References |
 |:---|:---:|:---:|:---:|:---:|:---:|:---:|
 | [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) | NeurIPS | '17 | NLP | μ„λ™μ£Ό | [π“](data/Attention%20is%20All%20You%20Need.pdf)|
 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929) | ICLR | '21 | CV | κΉ€λ‚ν› | [π“](data/presentation_vit.pdf) | 
 | [BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) | NAACL | '19 | NLP | μ¤μ„Έν™ | [π“](data/BERT.pdf) | 
 | [GPT: Improving Language Understanding by Generative Pre-Training](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf) |  | '18 | NLP | μ΄ν•μ¤€ | [π“](data/GPT.pdf)
 | [Pointer Networks](https://proceedings.neurips.cc/paper_files/paper/2015/hash/29921001f2f04bd3baee84a12e98098f-Abstract.html) | NeurIPS | '15 | | κΉ€ν„μ° | [π“](data/Pointer_Network.pdf) |
 | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) |  | '23 | NLP | κΉ€μ | [π“](data/LLaMA.pdf) |
 | [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864&hl=ja&sa=X&ei=5B0dZcHLGJ2h6rQPweSL0A0&scisig=AFWwaebUGjvb4JBysy2Z1l7aHWfJ&oi=scholarr) |  | '21 | NLP | μ¤μ„Έν™ | [π“](data/RoFormer%20_%20Enhanced%20Transformer%20with%20Rotary%20Position%20Embedding.pdf) | 
 | [Donut: Document Understanding Transformer without OCR](https://sangdooyun.github.io/data/kim2021donut.pdf) | ECCV | '22 | CV | κΉ€λ‚ν› | [π“](data/presentation_donut.pdf) | 
 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper) | ICCV | '21 | CV | μ„λ™μ£Ό | [π“](data/Swin%20Transformer.pdf) |
 | [Flamingo: A Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html) | NeurIPS | '22 | NLP | μ΄ν•μ¤€ | [π“](data/flamingo.pdf)
 | [Mistral AI](https://arxiv.org/pdf/2310.06825.pdf) |  | '23 | NLP | κΉ€μ | [π“](data/Mistral_7B.pdf) |
 | [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) | EMNLP | '19 | NLP | μ„λ™μ£Ό | [π“](data/Sentence_Bert.pdf) |
 | [ODIN: Enhancing the Reliability of Out-of-Distribution Image Detection in Neural Networks](https://arxiv.org/abs/1706.02690) | ICLR | '18 | OOD | κΉ€λ‚ν› | [π“](data/presentation_odin.pdf) | 
 | [Image Style Transfer using Convolutional Neural Networks](https://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html) | CVPR | '16 | CV | μ¤μ„Έν™ | [π“](data/Image%20Style%20Transfer%20Using%20Convolutional%20Neural%20Networks.pdf) | 
 | [wav2vec2.0: A Framework for Self-Supervised Learning of Speech Representations](https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html) | NeurIPS | '20 | STT | κΉ€μ | [π“](data/Wav2vec%202.0.pdf) |
 | [Can Large Language Models Be an Alternative to Human Evaluations?](https://arxiv.org/pdf/2305.01937.pdf) |  | '23 | NLP | μ΄ν•μ¤€ | [π“](data/LLMEvaluation.pdf)|
 | [Neural Speech Synthesis with Transformer Network](https://arxiv.org/pdf/1809.08895.pdf) | AAAI | '19 | TTS | μ„λ™μ£Ό | [π“](data/Neural_Speech_Synthesis_with_Transformer_Network.pdf) |
 | [Building robust Korean speech recognition model by fine-tuning large pretrained model](https://www.eksss.org/archive/view_article?doi=10.13064/KSSS.2023.15.3.075) | ν•κµ­μμ„±ν•™νμ§€ | '23 | STT | μµκ°‘μ£Ό | [π“](https://github.com/bear-stew/Presentation/blob/main/data/Korean%20speech%20recognition%20model%20by%20fine-tuning.pdf) |
 | [Make-A-Video: Text-to-Video Generation withou Text-Video Data](https://arxiv.org/abs/2209.14792) | | '22 | CV | κΉ€λ‚ν› | [π“](data/Make-A-Video.pdf) |
 | [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) | NeurIPS | '20 | CV | μ¤μ„Έν™ | [π“](data/Denoising%20Diffusion%20Probabilistic%20Models.pdf) |
 | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) | Openai | '21 | CV | μ΄ν•μ¤€ | [π“](data/CLIP.pdf) |
 | [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) |  | '22 | STT | κΉ€μ | [π“](data/Whisper.pdf) |
 | [TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data](https://arxiv.org/pdf/2201.07284.pdf) |  | '22 | Anomaly Detection | μ„λ™μ£Ό | [π“](data/TranAD.pdf) |
 | [Neural Machine Translation for Mathematical Formulae](https://arxiv.org/pdf/2305.16433.pdf) | ACL | '23 | NLP | μµκ°‘μ£Ό | [π“](data/Neural%20Machine%20Translation%20for%20Mathematical%20Formulae.pdf)
 | [Model-Agnostic Meta-Learning for Fast Adaption of Deep Networks](https://proceedings.mlr.press/v70/finn17a.html) | PMLR | '17 | Meta Learning | κΉ€λ‚ν› | [π“](data/Model-Agnostic%20Meta-Learning%20for%20Fast%20Adaption%20of%20Deep%20Networks.pdf) | [1](https://velog.io/@tobigs_xai/6%EC%A3%BC%EC%B0%A8-Introduction-to-Meta-Learning), [2](https://velog.io/@tobigs_xai/10%EC%A3%BC%EC%B0%A8-MAML-Model-agnostic-Meta-Learning-for-Fast-Adaptation-of-Deep-Networks-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0), [3](https://rhcsky.tistory.com/5) |
 | [Structured Denoising Diffusion Models in Discrete State-Spaces (D3PM)](https://arxiv.org/abs/2107.03006) | NeurIPS | '21 | LG | μ¤μ„Έν™ | [π“](data/Structured%20Denoising%20Diffusion%20Models%20in%20Discrete%20State-Spaces.pdf) |  
 | [Session-Based Recommendation with Graph Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/3804) | AAAI | '19 | Recommendation | μ„λ™μ£Ό | [π“](data/Session-Based%20Recommendation%20with%20Graph%20Neural%20Networks.pdf) |
 | [λ…Όλ¬Έμ λ©](#) | ν•™ν | 'μ—°λ„ | λ¶„μ•Ό | λ°ν‘μ | λ°ν‘μλ£ | μ°Έκ³ μλ£ |
